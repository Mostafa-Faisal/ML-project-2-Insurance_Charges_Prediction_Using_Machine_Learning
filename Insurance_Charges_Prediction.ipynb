{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacd93a0",
   "metadata": {},
   "source": [
    "# Insurance Charges Prediction Using Machine Learning\n",
    "\n",
    "## Project Overview\n",
    "**Goal**: Predict insurance charges based on client attributes such as age, BMI, smoking status, and other health factors.\n",
    "\n",
    "**Techniques Used**: Linear Regression, Random Forest, XGBoost\n",
    "\n",
    "**Dataset**: Insurance dataset with features like age, sex, BMI, children, smoker, region, and charges\n",
    "\n",
    "**Evaluation Metrics**: Mean Squared Error (MSE) and R¬≤ Score\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Information\n",
    "The dataset contains the following columns:\n",
    "- **age**: Age of the person\n",
    "- **sex**: Gender (male/female)\n",
    "- **bmi**: Body Mass Index\n",
    "- **children**: Number of children/dependents\n",
    "- **smoker**: Smoking status (yes/no)\n",
    "- **region**: Geographic region (northeast, northwest, southeast, southwest)\n",
    "- **charges**: Insurance charges (target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee57dbc",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install the necessary packages and import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn xgboost\n",
    "\n",
    "print(\"Required packages installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817740a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55620f6e",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Let's load the insurance dataset and explore its structure, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88557b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: Make sure you have downloaded the insurance.csv file from Kaggle\n",
    "# Dataset URL: https://www.kaggle.com/datasets/thedevastator/prediction-of-insurance-charges-using-age-gender\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('insurance.csv')\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset file not found. Please download 'insurance.csv' from Kaggle.\")\n",
    "    print(\"Dataset URL: https://www.kaggle.com/datasets/thedevastator/prediction-of-insurance-charges-using-age-gender\")\n",
    "    # Creating a sample dataset for demonstration\n",
    "    np.random.seed(42)\n",
    "    sample_data = {\n",
    "        'age': np.random.randint(18, 65, 100),\n",
    "        'sex': np.random.choice(['male', 'female'], 100),\n",
    "        'bmi': np.random.normal(28, 5, 100),\n",
    "        'children': np.random.randint(0, 5, 100),\n",
    "        'smoker': np.random.choice(['yes', 'no'], 100),\n",
    "        'region': np.random.choice(['northeast', 'northwest', 'southeast', 'southwest'], 100),\n",
    "        'charges': np.random.normal(13000, 5000, 100)\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"üìù Using sample dataset for demonstration purposes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8dc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüîç Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Unique Values in Categorical Columns:\")\n",
    "categorical_cols = ['sex', 'smoker', 'region']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809540ba",
   "metadata": {},
   "source": [
    "## 3. Visualize Data Distributions and Relationships\n",
    "\n",
    "Let's create various plots to understand the data distribution and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6250d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Insurance Dataset - Data Distribution and Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Age distribution\n",
    "axes[0, 0].hist(df['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. BMI distribution\n",
    "axes[0, 1].hist(df['bmi'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('BMI Distribution')\n",
    "axes[0, 1].set_xlabel('BMI')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Charges distribution\n",
    "axes[0, 2].hist(df['charges'], bins=20, alpha=0.7, color='salmon', edgecolor='black')\n",
    "axes[0, 2].set_title('Charges Distribution')\n",
    "axes[0, 2].set_xlabel('Charges ($)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Smoker vs Charges\n",
    "sns.boxplot(x='smoker', y='charges', data=df, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Smoker vs Charges')\n",
    "axes[1, 0].set_xlabel('Smoker')\n",
    "axes[1, 0].set_ylabel('Charges ($)')\n",
    "\n",
    "# 5. Region vs Charges\n",
    "sns.boxplot(x='region', y='charges', data=df, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Region vs Charges')\n",
    "axes[1, 1].set_xlabel('Region')\n",
    "axes[1, 1].set_ylabel('Charges ($)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Age vs Charges\n",
    "axes[1, 2].scatter(df['age'], df['charges'], alpha=0.6, color='purple')\n",
    "axes[1, 2].set_title('Age vs Charges')\n",
    "axes[1, 2].set_xlabel('Age')\n",
    "axes[1, 2].set_ylabel('Charges ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional insights\n",
    "print(\"üìä Key Insights from Visualizations:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚Ä¢ Age range: {df['age'].min()} - {df['age'].max()} years\")\n",
    "print(f\"‚Ä¢ BMI range: {df['bmi'].min():.1f} - {df['bmi'].max():.1f}\")\n",
    "print(f\"‚Ä¢ Charges range: ${df['charges'].min():.2f} - ${df['charges'].max():.2f}\")\n",
    "print(f\"‚Ä¢ Average charges: ${df['charges'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Smokers: {(df['smoker'] == 'yes').sum()} ({(df['smoker'] == 'yes').mean()*100:.1f}%)\")\n",
    "print(f\"‚Ä¢ Non-smokers: {(df['smoker'] == 'no').sum()} ({(df['smoker'] == 'no').mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea8e04",
   "metadata": {},
   "source": [
    "## 4. Preprocess Data (Encoding, Correlation)\n",
    "\n",
    "Now let's preprocess the data by encoding categorical variables and examining correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables using one-hot encoding\n",
    "print(\"üîÑ Processing categorical variables...\")\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "categorical_columns = ['sex', 'smoker', 'region']\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Encoded shape: {df_encoded.shape}\")\n",
    "print(f\"New columns after encoding: {list(df_encoded.columns)}\")\n",
    "\n",
    "# Display the first few rows of encoded data\n",
    "print(\"\\nüîç First 5 rows of encoded data:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Check for any missing values after encoding\n",
    "print(f\"\\n‚ùì Missing values after encoding: {df_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f71d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix and heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "plt.title('Correlation Matrix - Insurance Dataset Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find features most correlated with charges\n",
    "print(\"üîç Features most correlated with charges:\")\n",
    "print(\"=\" * 50)\n",
    "charges_correlation = correlation_matrix['charges'].abs().sort_values(ascending=False)\n",
    "for feature, corr in charges_correlation.items():\n",
    "    if feature != 'charges':\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Identify strongly correlated features\n",
    "print(f\"\\nüî• Features with correlation > 0.3 with charges:\")\n",
    "strong_corr = charges_correlation[charges_correlation > 0.3]\n",
    "strong_corr = strong_corr[strong_corr.index != 'charges']\n",
    "print(list(strong_corr.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d792d93",
   "metadata": {},
   "source": [
    "## 5. Prepare Features and Target Variables\n",
    "\n",
    "Let's separate our features and target variable, then split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "print(\"üéØ Preparing features and target variable...\")\n",
    "\n",
    "# Features (X) - all columns except charges\n",
    "X = df_encoded.drop('charges', axis=1)\n",
    "\n",
    "# Target variable (y) - charges column\n",
    "y = df_encoded['charges']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=None  # Since this is regression, not classification\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split Summary:\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set percentage: {(X_train.shape[0] / len(X)) * 100:.1f}%\")\n",
    "print(f\"Test set percentage: {(X_test.shape[0] / len(X)) * 100:.1f}%\")\n",
    "\n",
    "# Display basic statistics of the split\n",
    "print(f\"\\nüìà Target Variable Statistics:\")\n",
    "print(f\"Training set - Mean: ${y_train.mean():.2f}, Std: ${y_train.std():.2f}\")\n",
    "print(f\"Test set - Mean: ${y_test.mean():.2f}, Std: ${y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5333f9",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate Linear Regression Model\n",
    "\n",
    "Let's start with a Linear Regression model as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be953528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression Model\n",
    "print(\"ü§ñ Training Linear Regression Model...\")\n",
    "\n",
    "# Initialize the model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_pred_train = lr_model.predict(X_train)\n",
    "lr_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lr_mse_train = mean_squared_error(y_train, lr_pred_train)\n",
    "lr_mse_test = mean_squared_error(y_test, lr_pred_test)\n",
    "lr_r2_train = r2_score(y_train, lr_pred_train)\n",
    "lr_r2_test = r2_score(y_test, lr_pred_test)\n",
    "\n",
    "print(\"‚úÖ Linear Regression Model Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training MSE: {lr_mse_train:.2f}\")\n",
    "print(f\"Test MSE: {lr_mse_test:.2f}\")\n",
    "print(f\"Training R¬≤ Score: {lr_r2_train:.4f}\")\n",
    "print(f\"Test R¬≤ Score: {lr_r2_test:.4f}\")\n",
    "print(f\"Training RMSE: {np.sqrt(lr_mse_train):.2f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(lr_mse_test):.2f}\")\n",
    "\n",
    "# Display feature coefficients\n",
    "print(f\"\\nüîç Feature Coefficients:\")\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(feature_importance_lr)\n",
    "\n",
    "# Check for overfitting\n",
    "if lr_r2_train - lr_r2_test > 0.1:\n",
    "    print(\"‚ö†Ô∏è  Potential overfitting detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant overfitting detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122114f",
   "metadata": {},
   "source": [
    "## 7. Train and Evaluate Random Forest Model\n",
    "\n",
    "Now let's try a Random Forest model, which can capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Model\n",
    "print(\"üå≥ Training Random Forest Model...\")\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rf_mse_train = mean_squared_error(y_train, rf_pred_train)\n",
    "rf_mse_test = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_r2_train = r2_score(y_train, rf_pred_train)\n",
    "rf_r2_test = r2_score(y_test, rf_pred_test)\n",
    "\n",
    "print(\"‚úÖ Random Forest Model Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training MSE: {rf_mse_train:.2f}\")\n",
    "print(f\"Test MSE: {rf_mse_test:.2f}\")\n",
    "print(f\"Training R¬≤ Score: {rf_r2_train:.4f}\")\n",
    "print(f\"Test R¬≤ Score: {rf_r2_test:.4f}\")\n",
    "print(f\"Training RMSE: {np.sqrt(rf_mse_train):.2f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(rf_mse_test):.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüîç Feature Importance (Random Forest):\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# Check for overfitting\n",
    "if rf_r2_train - rf_r2_test > 0.1:\n",
    "    print(\"‚ö†Ô∏è  Potential overfitting detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant overfitting detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_rf, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importance - Random Forest Model', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 5 most important features\n",
    "print(\"üèÜ Top 5 Most Important Features:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (idx, row) in enumerate(feature_importance_rf.head().iterrows()):\n",
    "    print(f\"{i+1}. {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c1265",
   "metadata": {},
   "source": [
    "## 8. Train and Evaluate XGBoost Model\n",
    "\n",
    "Finally, let's try XGBoost, which is known for its excellent performance in many machine learning competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Model\n",
    "print(\"üöÄ Training XGBoost Model...\")\n",
    "\n",
    "# Initialize the model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred_train = xgb_model.predict(X_train)\n",
    "xgb_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "xgb_mse_train = mean_squared_error(y_train, xgb_pred_train)\n",
    "xgb_mse_test = mean_squared_error(y_test, xgb_pred_test)\n",
    "xgb_r2_train = r2_score(y_train, xgb_pred_train)\n",
    "xgb_r2_test = r2_score(y_test, xgb_pred_test)\n",
    "\n",
    "print(\"‚úÖ XGBoost Model Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training MSE: {xgb_mse_train:.2f}\")\n",
    "print(f\"Test MSE: {xgb_mse_test:.2f}\")\n",
    "print(f\"Training R¬≤ Score: {xgb_r2_train:.4f}\")\n",
    "print(f\"Test R¬≤ Score: {xgb_r2_test:.4f}\")\n",
    "print(f\"Training RMSE: {np.sqrt(xgb_mse_train):.2f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(xgb_mse_test):.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüîç Feature Importance (XGBoost):\")\n",
    "print(feature_importance_xgb)\n",
    "\n",
    "# Check for overfitting\n",
    "if xgb_r2_train - xgb_r2_test > 0.1:\n",
    "    print(\"‚ö†Ô∏è  Potential overfitting detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant overfitting detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bfe74",
   "metadata": {},
   "source": [
    "## 9. Compare Model Performance and Visualize Predictions\n",
    "\n",
    "Let's compare all three models and visualize their predictions against actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db876231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"üèÜ Model Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Train_MSE': [lr_mse_train, rf_mse_train, xgb_mse_train],\n",
    "    'Test_MSE': [lr_mse_test, rf_mse_test, xgb_mse_test],\n",
    "    'Train_R¬≤': [lr_r2_train, rf_r2_train, xgb_r2_train],\n",
    "    'Test_R¬≤': [lr_r2_test, rf_r2_test, xgb_r2_test],\n",
    "    'Train_RMSE': [np.sqrt(lr_mse_train), np.sqrt(rf_mse_train), np.sqrt(xgb_mse_train)],\n",
    "    'Test_RMSE': [np.sqrt(lr_mse_test), np.sqrt(rf_mse_test), np.sqrt(xgb_mse_test)]\n",
    "})\n",
    "\n",
    "print(results.round(4))\n",
    "\n",
    "# Find the best model based on test R¬≤ score\n",
    "best_model_idx = results['Test_R¬≤'].idxmax()\n",
    "best_model_name = results.loc[best_model_idx, 'Model']\n",
    "best_r2_score = results.loc[best_model_idx, 'Test_R¬≤']\n",
    "\n",
    "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "print(f\"Best Test R¬≤ Score: {best_r2_score:.4f}\")\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "baseline_r2 = results.loc[0, 'Test_R¬≤']  # Linear Regression as baseline\n",
    "if best_model_idx != 0:\n",
    "    improvement = ((best_r2_score - baseline_r2) / baseline_r2) * 100\n",
    "    print(f\"Improvement over Linear Regression: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Model Predictions vs Actual Values', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Linear Regression\n",
    "axes[0].scatter(y_test, lr_pred_test, alpha=0.6, color='blue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Charges ($)')\n",
    "axes[0].set_ylabel('Predicted Charges ($)')\n",
    "axes[0].set_title(f'Linear Regression\\n(R¬≤ = {lr_r2_test:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "axes[1].scatter(y_test, rf_pred_test, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Charges ($)')\n",
    "axes[1].set_ylabel('Predicted Charges ($)')\n",
    "axes[1].set_title(f'Random Forest\\n(R¬≤ = {rf_r2_test:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "axes[2].scatter(y_test, xgb_pred_test, alpha=0.6, color='orange')\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Charges ($)')\n",
    "axes[2].set_ylabel('Predicted Charges ($)')\n",
    "axes[2].set_title(f'XGBoost\\n(R¬≤ = {xgb_r2_test:.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Residual Plots (Actual - Predicted)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Linear Regression residuals\n",
    "lr_residuals = y_test - lr_pred_test\n",
    "axes[0].scatter(lr_pred_test, lr_residuals, alpha=0.6, color='blue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Charges ($)')\n",
    "axes[0].set_ylabel('Residuals ($)')\n",
    "axes[0].set_title('Linear Regression Residuals')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest residuals\n",
    "rf_residuals = y_test - rf_pred_test\n",
    "axes[1].scatter(rf_pred_test, rf_residuals, alpha=0.6, color='green')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Charges ($)')\n",
    "axes[1].set_ylabel('Residuals ($)')\n",
    "axes[1].set_title('Random Forest Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost residuals\n",
    "xgb_residuals = y_test - xgb_pred_test\n",
    "axes[2].scatter(xgb_pred_test, xgb_residuals, alpha=0.6, color='orange')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--')\n",
    "axes[2].set_xlabel('Predicted Charges ($)')\n",
    "axes[2].set_ylabel('Residuals ($)')\n",
    "axes[2].set_title('XGBoost Residuals')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69ecc5",
   "metadata": {},
   "source": [
    "## 10. Save the Best Model and Feature Columns\n",
    "\n",
    "Let's save the best performing model and feature columns for future use and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and feature columns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Determine the best model\n",
    "models = {\n",
    "    'Linear Regression': (lr_model, lr_r2_test),\n",
    "    'Random Forest': (rf_model, rf_r2_test),\n",
    "    'XGBoost': (xgb_model, xgb_r2_test)\n",
    "}\n",
    "\n",
    "best_model_name = max(models, key=lambda x: models[x][1])\n",
    "best_model = models[best_model_name][0]\n",
    "best_score = models[best_model_name][1]\n",
    "\n",
    "print(f\"üíæ Saving the best model: {best_model_name}\")\n",
    "print(f\"Best model R¬≤ score: {best_score:.4f}\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Save the best model\n",
    "with open('models/best_insurance_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save feature columns\n",
    "with open('models/feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "\n",
    "# Save model metadata\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'r2_score': best_score,\n",
    "    'mse_score': models[best_model_name][1],\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('models/model_info.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"üìÅ Files saved in 'models/' directory:\")\n",
    "print(\"  - best_insurance_model.pkl\")\n",
    "print(\"  - feature_columns.pkl\")\n",
    "print(\"  - model_info.pkl\")\n",
    "\n",
    "# Test loading the model\n",
    "print(\"\\nüîç Testing model loading...\")\n",
    "try:\n",
    "    with open('models/best_insurance_model.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    \n",
    "    with open('models/feature_columns.pkl', 'rb') as f:\n",
    "        loaded_features = pickle.load(f)\n",
    "    \n",
    "    # Test prediction\n",
    "    sample_prediction = loaded_model.predict(X_test.iloc[:1])\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Sample prediction: ${sample_prediction[0]:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad1d53",
   "metadata": {},
   "source": [
    "## üéâ Project Conclusion\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "This project successfully implemented three machine learning models to predict insurance charges:\n",
    "\n",
    "1. **Linear Regression** - Baseline model for comparison\n",
    "2. **Random Forest** - Ensemble method capturing non-linear relationships\n",
    "3. **XGBoost** - Gradient boosting algorithm\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Best Model**: The model with the highest R¬≤ score performed best\n",
    "- **Important Features**: Smoking status, BMI, and age were typically the most important predictors\n",
    "- **Model Performance**: All models showed reasonable performance in predicting insurance charges\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**: Use GridSearchCV or RandomizedSearchCV to optimize model parameters\n",
    "2. **Feature Engineering**: Create additional features like BMI categories, age groups\n",
    "3. **Cross-Validation**: Implement k-fold cross-validation for more robust evaluation\n",
    "4. **Deployment**: Create a web application using Streamlit or Flask\n",
    "5. **Monitoring**: Set up model monitoring in production\n",
    "\n",
    "### Deployment Instructions\n",
    "\n",
    "To deploy this model:\n",
    "1. Use the saved model files in the `models/` directory\n",
    "2. Create a web application using Streamlit (see the step-by-step guide)\n",
    "3. Handle categorical encoding consistently with training data\n",
    "4. Validate input data before making predictions\n",
    "\n",
    "**Great job completing this insurance charges prediction project! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a7850",
   "metadata": {},
   "source": [
    "## üöÄ Streamlit Deployment\n",
    "\n",
    "Now that we have trained and saved our model, let's deploy it using Streamlit for interactive predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit Deployment Instructions\n",
    "print(\"üöÄ STREAMLIT DEPLOYMENT READY!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if model files exist\n",
    "import os\n",
    "model_files = ['models/best_insurance_model.pkl', 'models/feature_columns.pkl', 'models/model_info.pkl']\n",
    "all_files_exist = all(os.path.exists(file) for file in model_files)\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"‚úÖ All model files are ready for deployment!\")\n",
    "    print(\"\\nüìÅ Required files found:\")\n",
    "    for file in model_files:\n",
    "        size = os.path.getsize(file) / 1024  # Size in KB\n",
    "        print(f\"  ‚Ä¢ {file} ({size:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüéØ To deploy your model with Streamlit:\")\n",
    "    print(\"1. Install Streamlit: pip install streamlit\")\n",
    "    print(\"2. Run the app: streamlit run streamlit_app.py\")\n",
    "    print(\"3. Or use the batch file: run_app.bat\")\n",
    "    print(\"\\nüåê Your app will open at: http://localhost:8501\")\n",
    "    \n",
    "    print(f\"\\nüìä Model Information:\")\n",
    "    try:\n",
    "        with open('models/model_info.pkl', 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        print(f\"  ‚Ä¢ Best Model: {model_info['model_name']}\")\n",
    "        print(f\"  ‚Ä¢ R¬≤ Score: {model_info['r2_score']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Training Date: {model_info['training_date']}\")\n",
    "    except:\n",
    "        print(\"  ‚Ä¢ Model info available in deployment\")\n",
    "    \n",
    "    print(f\"\\nüé® Streamlit App Features:\")\n",
    "    print(\"  ‚Ä¢ Interactive input forms\")\n",
    "    print(\"  ‚Ä¢ Real-time predictions\")\n",
    "    print(\"  ‚Ä¢ Risk assessment\")\n",
    "    print(\"  ‚Ä¢ Feature importance charts\")\n",
    "    print(\"  ‚Ä¢ Modern, responsive design\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Some model files are missing!\")\n",
    "    print(\"Please make sure all previous cells have been executed successfully.\")\n",
    "    \n",
    "print(f\"\\nüìù Next Steps:\")\n",
    "print(\"1. Open terminal/command prompt\")\n",
    "print(\"2. Navigate to this directory\")\n",
    "print(\"3. Run: streamlit run streamlit_app.py\")\n",
    "print(\"4. Enjoy your deployed ML model! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
